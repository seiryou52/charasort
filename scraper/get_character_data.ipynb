{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6614910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1cf03",
   "metadata": {},
   "source": [
    "* Assumes wiki is gold standard source of truth and relies on its page structure, chronology, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bc3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://toarumajutsunoindex.fandom.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5dddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 'Toaru_Majutsu_no_Index'\n",
    "RAILGUN = 'Toaru_Kagaku_no_Railgun'\n",
    "ASTRAL_BUDDY = 'Astral_Buddy'\n",
    "ACCEL = 'Toaru_Kagaku_no_Accelerator'\n",
    "DARK_MATTER = 'Toaru_Kagaku_no_Dark_Matter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5799ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_data(wiki_url, affiliation):\n",
    "        \n",
    "    data = {\n",
    "        'name_en': '',\n",
    "        'name_jp': '',\n",
    "        'img_url': '',\n",
    "        'affiliation': [affiliation],\n",
    "        'series': [],\n",
    "        'is_supporting_character': False\n",
    "    }\n",
    "    \n",
    "    soup = BeautifulSoup(requests.get(wiki_url).text, 'html.parser')\n",
    "    name_en = soup.select('h1.page-header__title')[0].text\n",
    "    name_jp_raw = soup.find(\"div\", {\"data-source\": \"Kanji\"})\n",
    "    \n",
    "    # only get characters with both japanese and english names\n",
    "    if not name_jp_raw:\n",
    "        return None\n",
    "    \n",
    "    data['name_en'] = name_en\n",
    "    data['name_jp'] = name_jp_raw.find(\"div\", {\"class\": \"pi-data-value pi-font\"}).text\n",
    "    \n",
    "    # default image\n",
    "    default_img_url = soup.find(\"a\", {\"class\": \"image-thumbnail\"}).get('href')\n",
    "    data['img_url'] = default_img_url\n",
    "    \n",
    "    headlines = [_['id'] for _ in soup.select(\"span.mw-headline\")]\n",
    "    for headline in headlines:\n",
    "        if INDEX in headline:\n",
    "            data['series'].append('禁書')\n",
    "        elif RAILGUN in headline:\n",
    "            data['series'].append('超電磁砲')\n",
    "        elif ASTRAL_BUDDY in headline:\n",
    "            data['series'].append('アストラル・バディ')\n",
    "        elif ACCEL in headline:\n",
    "            data['series'].append('一方通行')\n",
    "        elif DARK_MATTER in headline:\n",
    "            data['series'].append('未元物質')\n",
    "\n",
    "    # ignore characters that are not in any of the main series/spinoffs\n",
    "    if not data['series']:\n",
    "        return None\n",
    "\n",
    "    intro_text = ''.join(_ for _ in [m.get('content') for m in soup.find_all(\"meta\")] if _)\n",
    "    # heuristics to find minor characters\n",
    "    is_stub = soup.find('div', {'id': 'stub'}) is not None\n",
    "    is_few_sections = len(soup.find_all('li', {'class': 'toclevel-2'})) < 3\n",
    "    # explicit mentions\n",
    "    minor_pattern = r'\\bis a (small )?(minor|supporting|background|side)\\b'\n",
    "    is_minor = bool(re.search(minor_pattern, intro_text, re.IGNORECASE))\n",
    "    data['is_supporting_character'] = is_stub or is_few_sections or is_minor\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb56ce",
   "metadata": {},
   "source": [
    "* Affiliation (magic/science/other) based on wiki categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87dafa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'magic': {\n",
    "        'path': '/wiki/Category:Magic_Side_Characters',\n",
    "        'char_urls': []\n",
    "    },\n",
    "    'science': {\n",
    "        'path': '/wiki/Category:Science_Side_Characters',\n",
    "        'char_urls': []\n",
    "    },\n",
    "    'other': {\n",
    "        'path': '/wiki/Category:Normal_Characters',\n",
    "        'char_urls': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3509c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_pages(url, char_urls=None):\n",
    "    if char_urls is None:\n",
    "        char_urls = []\n",
    "    \n",
    "    curr_page = requests.get(url, allow_redirects=False)\n",
    "    assert(curr_page.status_code == 200)\n",
    "    \n",
    "    curr_soup = BeautifulSoup(curr_page.text, 'html.parser')\n",
    "    \n",
    "    # only get characters with pictures\n",
    "    curr_char_divs = [\n",
    "        div.find('a') for div in curr_soup.find_all(\"div\", {\"class\": \"category-page__member-left\"})\n",
    "        if 'Template_Placeholder_other.png' not in str(div)\n",
    "    ]\n",
    "    curr_char_divs = [d for d in curr_char_divs if d]\n",
    "\n",
    "    # only get characters with valid pages (& no redirects)\n",
    "    for curr_char_div in curr_char_divs:\n",
    "        curr_char_url = \"{base}{suffix}\".format(base=base_url, suffix=curr_char_div.get('href'))\n",
    "        is_valid_page = requests.get(curr_char_url, allow_redirects=False).status_code == 200\n",
    "        if not is_valid_page:\n",
    "            continue\n",
    "        char_urls.append(curr_char_url)\n",
    "\n",
    "    next_page = curr_soup.find(\"a\", {\"class\": \"category-page__pagination-next\"})\n",
    "    if next_page:\n",
    "        next_url = next_page.get(\"href\")\n",
    "        char_urls.extend(get_character_pages(next_url, char_urls))\n",
    "        \n",
    "    return char_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6f6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, _ in categories.items():\n",
    "    category_url = base_url + _['path']\n",
    "    categories[name]['char_urls'] = get_character_pages(category_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f46e6",
   "metadata": {},
   "source": [
    "Get data\n",
    "\n",
    "* Deal format is array of characters\n",
    "* Target json - [{name:\"name\", img: \"img.png\", opts: {series:[\"a\"], affiliation:[\"b\"]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e59a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character:\n",
    "    def __init__(self):\n",
    "        self.name = \"\"\n",
    "        self.affiliation = set()\n",
    "        self.series = set()\n",
    "        self.is_supporting_character = False\n",
    "        self.img = \"\"\n",
    "    \n",
    "    def get_data(self):\n",
    "        return dict(\n",
    "            name = self.name,\n",
    "            img = self.img,\n",
    "            opts = dict(\n",
    "                affiliation = list(self.affiliation),\n",
    "                series = list(self.series),\n",
    "                is_supporting_character = self.is_supporting_character\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88bb97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = dict()\n",
    "\n",
    "for category, _ in categories.items():\n",
    "    for char_url in _['char_urls']:\n",
    "        \n",
    "        curr_data = get_character_data(char_url, category)\n",
    "        if not curr_data:\n",
    "            continue\n",
    "                \n",
    "        name = \"{jp} ({en})\".format(jp=curr_data['name_jp'], en=curr_data['name_en'])\n",
    "        \n",
    "        if name in characters:\n",
    "            curr_char = characters[name]\n",
    "        else:    \n",
    "            curr_char = Character()\n",
    "            curr_char.name = name\n",
    "            # note that .png extension comes after postprocessing\n",
    "            curr_char.img = re.search('latest\\?cb=(.+)', curr_data['img_url'], re.IGNORECASE).group(1) + '.png'\n",
    "            #curr_char.img = curr_data['img_url']\n",
    "            curr_char.affiliation.update(curr_data['affiliation'])\n",
    "            curr_char.series.update(curr_data['series'])\n",
    "            curr_char.is_supporting_character = curr_data['is_supporting_character']\n",
    "        \n",
    "        characters[name] = curr_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94403b98",
   "metadata": {},
   "source": [
    "# TODO if only appear in SS then minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dffa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some manual updates until i figure out better heuristics\n",
    "chars_in_ab = ['御坂 美琴 (Misaka Mikoto)', '食蜂 操祈 (Shokuhou Misaki)', \n",
    "               '白井 黒子 (Shirai Kuroko)', '初春 飾利 (Uiharu Kazari)', '佐天 涙子 (Saten Ruiko)']\n",
    "chars_not_minor = ['初春 飾利 (Uiharu Kazari)', '佐天 涙子 (Saten Ruiko)', \"神苑小路 瑠璃懸巣 (Shin'enkouji Rurikakesu)\",\n",
    "                   '婚后 光子 (Kongou Mitsuko)', '湾内 絹保 (Wannai Kinuho)', '御坂 美鈴 (Misaka Misuzu)']\n",
    "for char in chars_in_ab:\n",
    "    characters[char].series.update([\"アストラル・バディ\"])\n",
    "for char in chars_not_minor:\n",
    "    characters[char].is_supporting_character = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a492f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [character.get_data() for character in characters.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27131011",
   "metadata": {},
   "source": [
    "Ended up downloading the images using wget ._.\n",
    "\n",
    "To preserve aspect ratio, the downloaded images were resized and padded instead. See the other notebook in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23aaf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_image_urls = False\n",
    "if write_image_urls:\n",
    "    with open('img.txt', 'w') as f:\n",
    "        for _ in output_data:\n",
    "            f.write(_['img'])\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
